---
permalink: /
title: "üöÄ Welcome to Quan's homepage!"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
Hi, thanks for visiting! My name is Quan. 

I am currently an AI Resident at [FPT AI Residency](https://wp.fpt-aic.com/ai-residency/), working under the supervision of [Dr. Hoang Thanh-Tung](https://scholar.google.com/citations?user=xZU08d0AAAAJ). My research focuses on exploring alternatives that challenge the dominance of autoregressive Transformers, with a particular emphasis on non-autoregressive language models and state space models. I hold both a Master‚Äôs and a Bachelor‚Äôs degree in Computer Science from the [Hanoi University of Science and Technology](https://hust.edu.vn/en/). Throughout both my undergraduate and Master's studies, I conducted research on the problem of Continual Learning under the supervision of [Dr. Linh Ngo Van](https://scholar.google.com/citations?user=tZ78MoQAAAAJ) and [Associate Prof. Khoat Than](https://users.soict.hust.edu.vn/khoattq/). Since August 2023, I have had the privilege of working under the supervision of [Professor Long Tran-Thanh](https://warwick.ac.uk/fac/sci/dcs/people/long_tran-thanh/) and [Professor Hongkai Wen](https://hongkaiw.github.io/). Under their guidance, I have conducted research on Pruning at Initialization and Neural Architecture Search, developing a differentiable algorithm to address the discrete optimization challenges inherent in both problems.

üî¨ Research interest
======
I am still in the early stages of my academic journey, where I actively seek opportunities for guidance that allow me to delve deeply into intellectually intriguing and highly impactful researches. Currently, my research spans a wide range of topics within the Efficient AI domain, where I focus on enhancing the training efficiency of deep learning models and reducing the inference latency of language models. These innovations aim to lower the computational cost of training and inference, making AI systems more accessible and deployable in resource-constrained environments.

My research experience includes the following areas:
* Text-to-Image Diffusion Models
* Diffusion Language Models
* Non-Autoregressive Transformer
* State Space Models
* Neural Networks Compression
* Pruning at Initialization
* Neural Architecture Search
* Knowledge Distillation
* Continual Learning
* Few-shot Learning
* Meta Learning

üóûÔ∏è News
======

* [*May 01, 2025*] Our new paper, [Provably Improving Generalization of Few-Shot Models with Synthetic Data](http://QuanNguyen-Tri.github.io/files/6937_Provably_Improving_Genera.pdf), has been accepted to [ICML 2025](https://icml.cc/).
* [*Mar 05, 2025*] Our new paper, [Large Language Models powered Neural Solvers for Generalized Vehicle Routing Problems](https://openreview.net/pdf?id=EVqlVjvlt8), has been accepted to [Agentic AI for Scientific Discovery](https://iclragenticai.github.io/)
* [*Jan 22, 2025*] Our new paper, [DPaI: Differentiable Pruning at Initialization with Node-Path Balance Principle](https://openreview.net/pdf?id=hvLBTpiDt3), has been accepted to [ICLR 2025](https://iclr.cc/).

üß¨ About Myself 
======

Supervisor 
======


